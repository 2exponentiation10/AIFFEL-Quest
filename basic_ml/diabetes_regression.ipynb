{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "77fec756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요 모듈 import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479c50fa",
   "metadata": {},
   "source": [
    "### (1) 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f7d387d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn의 내장 데이터 활용\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "diabetes=load_diabetes(as_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a7a4c411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 데이터와 target 데이터 데이터프레임 형태로 정의\n",
    "df_x = diabetes.data\n",
    "df_y = diabetes.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c5ba476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442, 10)\n",
      "(442,)\n"
     ]
    }
   ],
   "source": [
    "print(df_x.shape)\n",
    "print(df_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "01d4b77f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019908</td>\n",
       "      <td>-0.017646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068330</td>\n",
       "      <td>-0.092204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005671</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002864</td>\n",
       "      <td>-0.025930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022692</td>\n",
       "      <td>-0.009362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031991</td>\n",
       "      <td>-0.046641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age       sex       bmi        bp        s1        s2        s3  \\\n",
       "0  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
       "1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
       "2  0.085299  0.050680  0.044451 -0.005671 -0.045599 -0.034194 -0.032356   \n",
       "3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
       "4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
       "\n",
       "         s4        s5        s6  \n",
       "0 -0.002592  0.019908 -0.017646  \n",
       "1 -0.039493 -0.068330 -0.092204  \n",
       "2 -0.002592  0.002864 -0.025930  \n",
       "3  0.034309  0.022692 -0.009362  \n",
       "4 -0.002592 -0.031991 -0.046641  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature, values 확인\n",
    "df_x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f96ae8f",
   "metadata": {},
   "source": [
    "### (2)(3) 모델에 입력할 데이터 X, y 준비하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d5660b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe to ndarray\n",
    "X = df_x.values\n",
    "y = df_y.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fa14da",
   "metadata": {},
   "source": [
    "### (4) train 데이터와 test 데이터로 분리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "301723ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(331, 10) (331,)\n",
      "(111, 10) (111,)\n"
     ]
    }
   ],
   "source": [
    "# train dataset, test dataset 분리\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)    # default: 25%\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559b9942",
   "metadata": {},
   "source": [
    "### (5) 모델 준비하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fb315a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤하게 W, b 초기화\n",
    "W = np.random.rand(X_train.shape[1])    # feature 개수만큼 있어야 함\n",
    "b = np.random.rand()    # 1개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1e6c8dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선형 회귀 모델\n",
    "def model(X, W, b):\n",
    "    predictions = 0\n",
    "    for i in range(X_train.shape[1]):    # 가중치 개수만큼 반복되어야 함\n",
    "        predictions += X[:, i] * W[i]\n",
    "    predictions += b\n",
    "    return predictions    # 현재의 W, b 값을 바탕으로 예측값 산출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd7c424",
   "metadata": {},
   "source": [
    "### (6) 손실함수 loss 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e8a2da37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE: 오차 제곱의 평균\n",
    "def MSE(a, b):\n",
    "    mse = ((a - b) ** 2).mean()  # 두 값의 차이의 제곱의 평균\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "23521162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측값과 실제 정답 간의 loss\n",
    "def loss(X, W, b, y):\n",
    "    predictions = model(X, W, b)\n",
    "    L = MSE(predictions, y)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c261b13d",
   "metadata": {},
   "source": [
    "### (7) 기울기를 구하는 gradient 함수 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4358d8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미분 공식을 바탕으로 gradient 함수 정의\n",
    "def gradient(X, W, b, y):\n",
    "    N = len(y)\n",
    "    \n",
    "    y_pred = model(X, W, b)\n",
    "    \n",
    "    # loss function을 각각 W와 b로 편미분\n",
    "    dW = 1/N * 2 * X.T.dot(y_pred - y)\n",
    "    db = 2 * (y_pred - y).mean()\n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd06526",
   "metadata": {},
   "source": [
    "### (8) 하이퍼 파라미터인 학습률 설정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0a432c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습률: 0.0001, 0.01\n",
    "LEARNING_RATE = [0.0001, 0.01]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18afc702",
   "metadata": {},
   "source": [
    "### (9) 모델 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e55e8ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================\n",
      "학습률: 0.0001, 총 epoch: 1001\n",
      "Iteration 100 : Loss 28173.0236\n",
      "Iteration 200 : Loss 27302.2139\n",
      "Iteration 300 : Loss 26465.5226\n",
      "Iteration 400 : Loss 25661.6120\n",
      "Iteration 500 : Loss 24889.1964\n",
      "Iteration 600 : Loss 24147.0409\n",
      "Iteration 700 : Loss 23433.9587\n",
      "Iteration 800 : Loss 22748.8098\n",
      "Iteration 900 : Loss 22090.4989\n",
      "Iteration 1000 : Loss 21457.9733\n",
      "====================================\n",
      "학습률: 0.0001, 총 epoch: 10001\n",
      "Iteration 100 : Loss 28165.1096\n",
      "Iteration 200 : Loss 27294.3075\n",
      "Iteration 300 : Loss 26457.6237\n",
      "Iteration 400 : Loss 25653.7203\n",
      "Iteration 500 : Loss 24881.3119\n",
      "Iteration 600 : Loss 24139.1632\n",
      "Iteration 700 : Loss 23426.0878\n",
      "Iteration 800 : Loss 22740.9455\n",
      "Iteration 900 : Loss 22082.6409\n",
      "Iteration 1000 : Loss 21450.1216\n",
      "Iteration 1100 : Loss 20842.3765\n",
      "Iteration 1200 : Loss 20258.4340\n",
      "Iteration 1300 : Loss 19697.3607\n",
      "Iteration 1400 : Loss 19158.2598\n",
      "Iteration 1500 : Loss 18640.2697\n",
      "Iteration 1600 : Loss 18142.5626\n",
      "Iteration 1700 : Loss 17664.3430\n",
      "Iteration 1800 : Loss 17204.8467\n",
      "Iteration 1900 : Loss 16763.3396\n",
      "Iteration 2000 : Loss 16339.1162\n",
      "Iteration 2100 : Loss 15931.4986\n",
      "Iteration 2200 : Loss 15539.8359\n",
      "Iteration 2300 : Loss 15163.5022\n",
      "Iteration 2400 : Loss 14801.8964\n",
      "Iteration 2500 : Loss 14454.4411\n",
      "Iteration 2600 : Loss 14120.5812\n",
      "Iteration 2700 : Loss 13799.7838\n",
      "Iteration 2800 : Loss 13491.5365\n",
      "Iteration 2900 : Loss 13195.3472\n",
      "Iteration 3000 : Loss 12910.7430\n",
      "Iteration 3100 : Loss 12637.2697\n",
      "Iteration 3200 : Loss 12374.4908\n",
      "Iteration 3300 : Loss 12121.9869\n",
      "Iteration 3400 : Loss 11879.3550\n",
      "Iteration 3500 : Loss 11646.2081\n",
      "Iteration 3600 : Loss 11422.1742\n",
      "Iteration 3700 : Loss 11206.8959\n",
      "Iteration 3800 : Loss 11000.0299\n",
      "Iteration 3900 : Loss 10801.2464\n",
      "Iteration 4000 : Loss 10610.2284\n",
      "Iteration 4100 : Loss 10426.6713\n",
      "Iteration 4200 : Loss 10250.2827\n",
      "Iteration 4300 : Loss 10080.7813\n",
      "Iteration 4400 : Loss 9917.8971\n",
      "Iteration 4500 : Loss 9761.3707\n",
      "Iteration 4600 : Loss 9610.9528\n",
      "Iteration 4700 : Loss 9466.4037\n",
      "Iteration 4800 : Loss 9327.4934\n",
      "Iteration 4900 : Loss 9194.0007\n",
      "Iteration 5000 : Loss 9065.7132\n",
      "Iteration 5100 : Loss 8942.4269\n",
      "Iteration 5200 : Loss 8823.9455\n",
      "Iteration 5300 : Loss 8710.0806\n",
      "Iteration 5400 : Loss 8600.6513\n",
      "Iteration 5500 : Loss 8495.4836\n",
      "Iteration 5600 : Loss 8394.4104\n",
      "Iteration 5700 : Loss 8297.2712\n",
      "Iteration 5800 : Loss 8203.9116\n",
      "Iteration 5900 : Loss 8114.1835\n",
      "Iteration 6000 : Loss 8027.9445\n",
      "Iteration 6100 : Loss 7945.0577\n",
      "Iteration 6200 : Loss 7865.3917\n",
      "Iteration 6300 : Loss 7788.8202\n",
      "Iteration 6400 : Loss 7715.2219\n",
      "Iteration 6500 : Loss 7644.4802\n",
      "Iteration 6600 : Loss 7576.4831\n",
      "Iteration 6700 : Loss 7511.1229\n",
      "Iteration 6800 : Loss 7448.2963\n",
      "Iteration 6900 : Loss 7387.9039\n",
      "Iteration 7000 : Loss 7329.8503\n",
      "Iteration 7100 : Loss 7274.0437\n",
      "Iteration 7200 : Loss 7220.3961\n",
      "Iteration 7300 : Loss 7168.8227\n",
      "Iteration 7400 : Loss 7119.2424\n",
      "Iteration 7500 : Loss 7071.5768\n",
      "Iteration 7600 : Loss 7025.7510\n",
      "Iteration 7700 : Loss 6981.6928\n",
      "Iteration 7800 : Loss 6939.3329\n",
      "Iteration 7900 : Loss 6898.6047\n",
      "Iteration 8000 : Loss 6859.4442\n",
      "Iteration 8100 : Loss 6821.7900\n",
      "Iteration 8200 : Loss 6785.5830\n",
      "Iteration 8300 : Loss 6750.7665\n",
      "Iteration 8400 : Loss 6717.2858\n",
      "Iteration 8500 : Loss 6685.0888\n",
      "Iteration 8600 : Loss 6654.1250\n",
      "Iteration 8700 : Loss 6624.3460\n",
      "Iteration 8800 : Loss 6595.7055\n",
      "Iteration 8900 : Loss 6568.1588\n",
      "Iteration 9000 : Loss 6541.6630\n",
      "Iteration 9100 : Loss 6516.1769\n",
      "Iteration 9200 : Loss 6491.6610\n",
      "Iteration 9300 : Loss 6468.0771\n",
      "Iteration 9400 : Loss 6445.3887\n",
      "Iteration 9500 : Loss 6423.5608\n",
      "Iteration 9600 : Loss 6402.5596\n",
      "Iteration 9700 : Loss 6382.3527\n",
      "Iteration 9800 : Loss 6362.9090\n",
      "Iteration 9900 : Loss 6344.1985\n",
      "Iteration 10000 : Loss 6326.1925\n",
      "====================================\n",
      "학습률: 0.0001, 총 epoch: 30001\n",
      "Iteration 100 : Loss 28087.9986\n",
      "Iteration 200 : Loss 27217.2489\n",
      "Iteration 300 : Loss 26380.6164\n",
      "Iteration 400 : Loss 25576.7632\n",
      "Iteration 500 : Loss 24804.4039\n",
      "Iteration 600 : Loss 24062.3034\n",
      "Iteration 700 : Loss 23349.2752\n",
      "Iteration 800 : Loss 22664.1793\n",
      "Iteration 900 : Loss 22005.9201\n",
      "Iteration 1000 : Loss 21373.4455\n",
      "Iteration 1100 : Loss 20765.7441\n",
      "Iteration 1200 : Loss 20181.8447\n",
      "Iteration 1300 : Loss 19620.8136\n",
      "Iteration 1400 : Loss 19081.7543\n",
      "Iteration 1500 : Loss 18563.8051\n",
      "Iteration 1600 : Loss 18066.1382\n",
      "Iteration 1700 : Loss 17587.9581\n",
      "Iteration 1800 : Loss 17128.5008\n",
      "Iteration 1900 : Loss 16687.0321\n",
      "Iteration 2000 : Loss 16262.8464\n",
      "Iteration 2100 : Loss 15855.2661\n",
      "Iteration 2200 : Loss 15463.6401\n",
      "Iteration 2300 : Loss 15087.3426\n",
      "Iteration 2400 : Loss 14725.7726\n",
      "Iteration 2500 : Loss 14378.3525\n",
      "Iteration 2600 : Loss 14044.5275\n",
      "Iteration 2700 : Loss 13723.7644\n",
      "Iteration 2800 : Loss 13415.5511\n",
      "Iteration 2900 : Loss 13119.3954\n",
      "Iteration 3000 : Loss 12834.8244\n",
      "Iteration 3100 : Loss 12561.3839\n",
      "Iteration 3200 : Loss 12298.6374\n",
      "Iteration 3300 : Loss 12046.1656\n",
      "Iteration 3400 : Loss 11803.5656\n",
      "Iteration 3500 : Loss 11570.4501\n",
      "Iteration 3600 : Loss 11346.4474\n",
      "Iteration 3700 : Loss 11131.2000\n",
      "Iteration 3800 : Loss 10924.3647\n",
      "Iteration 3900 : Loss 10725.6115\n",
      "Iteration 4000 : Loss 10534.6236\n",
      "Iteration 4100 : Loss 10351.0963\n",
      "Iteration 4200 : Loss 10174.7372\n",
      "Iteration 4300 : Loss 10005.2652\n",
      "Iteration 4400 : Loss 9842.4102\n",
      "Iteration 4500 : Loss 9685.9128\n",
      "Iteration 4600 : Loss 9535.5235\n",
      "Iteration 4700 : Loss 9391.0030\n",
      "Iteration 4800 : Loss 9252.1210\n",
      "Iteration 4900 : Loss 9118.6564\n",
      "Iteration 5000 : Loss 8990.3970\n",
      "Iteration 5100 : Loss 8867.1384\n",
      "Iteration 5200 : Loss 8748.6846\n",
      "Iteration 5300 : Loss 8634.8473\n",
      "Iteration 5400 : Loss 8525.4453\n",
      "Iteration 5500 : Loss 8420.3048\n",
      "Iteration 5600 : Loss 8319.2586\n",
      "Iteration 5700 : Loss 8222.1463\n",
      "Iteration 5800 : Loss 8128.8135\n",
      "Iteration 5900 : Loss 8039.1120\n",
      "Iteration 6000 : Loss 7952.8995\n",
      "Iteration 6100 : Loss 7870.0391\n",
      "Iteration 6200 : Loss 7790.3994\n",
      "Iteration 6300 : Loss 7713.8542\n",
      "Iteration 6400 : Loss 7640.2820\n",
      "Iteration 6500 : Loss 7569.5662\n",
      "Iteration 6600 : Loss 7501.5950\n",
      "Iteration 6700 : Loss 7436.2606\n",
      "Iteration 6800 : Loss 7373.4597\n",
      "Iteration 6900 : Loss 7313.0929\n",
      "Iteration 7000 : Loss 7255.0648\n",
      "Iteration 7100 : Loss 7199.2837\n",
      "Iteration 7200 : Loss 7145.6615\n",
      "Iteration 7300 : Loss 7094.1134\n",
      "Iteration 7400 : Loss 7044.5583\n",
      "Iteration 7500 : Loss 6996.9179\n",
      "Iteration 7600 : Loss 6951.1172\n",
      "Iteration 7700 : Loss 6907.0840\n",
      "Iteration 7800 : Loss 6864.7491\n",
      "Iteration 7900 : Loss 6824.0458\n",
      "Iteration 8000 : Loss 6784.9102\n",
      "Iteration 8100 : Loss 6747.2807\n",
      "Iteration 8200 : Loss 6711.0984\n",
      "Iteration 8300 : Loss 6676.3066\n",
      "Iteration 8400 : Loss 6642.8506\n",
      "Iteration 8500 : Loss 6610.6781\n",
      "Iteration 8600 : Loss 6579.7388\n",
      "Iteration 8700 : Loss 6549.9844\n",
      "Iteration 8800 : Loss 6521.3683\n",
      "Iteration 8900 : Loss 6493.8460\n",
      "Iteration 9000 : Loss 6467.3746\n",
      "Iteration 9100 : Loss 6441.9128\n",
      "Iteration 9200 : Loss 6417.4211\n",
      "Iteration 9300 : Loss 6393.8614\n",
      "Iteration 9400 : Loss 6371.1973\n",
      "Iteration 9500 : Loss 6349.3936\n",
      "Iteration 9600 : Loss 6328.4165\n",
      "Iteration 9700 : Loss 6308.2337\n",
      "Iteration 9800 : Loss 6288.8140\n",
      "Iteration 9900 : Loss 6270.1275\n",
      "Iteration 10000 : Loss 6252.1455\n",
      "Iteration 10100 : Loss 6234.8403\n",
      "Iteration 10200 : Loss 6218.1855\n",
      "Iteration 10300 : Loss 6202.1554\n",
      "Iteration 10400 : Loss 6186.7257\n",
      "Iteration 10500 : Loss 6171.8728\n",
      "Iteration 10600 : Loss 6157.5740\n",
      "Iteration 10700 : Loss 6143.8077\n",
      "Iteration 10800 : Loss 6130.5530\n",
      "Iteration 10900 : Loss 6117.7899\n",
      "Iteration 11000 : Loss 6105.4990\n",
      "Iteration 11100 : Loss 6093.6618\n",
      "Iteration 11200 : Loss 6082.2607\n",
      "Iteration 11300 : Loss 6071.2784\n",
      "Iteration 11400 : Loss 6060.6986\n",
      "Iteration 11500 : Loss 6050.5055\n",
      "Iteration 11600 : Loss 6040.6839\n",
      "Iteration 11700 : Loss 6031.2193\n",
      "Iteration 11800 : Loss 6022.0976\n",
      "Iteration 11900 : Loss 6013.3056\n",
      "Iteration 12000 : Loss 6004.8301\n",
      "Iteration 12100 : Loss 5996.6589\n",
      "Iteration 12200 : Loss 5988.7800\n",
      "Iteration 12300 : Loss 5981.1819\n",
      "Iteration 12400 : Loss 5973.8537\n",
      "Iteration 12500 : Loss 5966.7848\n",
      "Iteration 12600 : Loss 5959.9649\n",
      "Iteration 12700 : Loss 5953.3844\n",
      "Iteration 12800 : Loss 5947.0339\n",
      "Iteration 12900 : Loss 5940.9044\n",
      "Iteration 13000 : Loss 5934.9871\n",
      "Iteration 13100 : Loss 5929.2739\n",
      "Iteration 13200 : Loss 5923.7567\n",
      "Iteration 13300 : Loss 5918.4278\n",
      "Iteration 13400 : Loss 5913.2798\n",
      "Iteration 13500 : Loss 5908.3057\n",
      "Iteration 13600 : Loss 5903.4986\n",
      "Iteration 13700 : Loss 5898.8521\n",
      "Iteration 13800 : Loss 5894.3598\n",
      "Iteration 13900 : Loss 5890.0156\n",
      "Iteration 14000 : Loss 5885.8139\n",
      "Iteration 14100 : Loss 5881.7490\n",
      "Iteration 14200 : Loss 5877.8155\n",
      "Iteration 14300 : Loss 5874.0083\n",
      "Iteration 14400 : Loss 5870.3225\n",
      "Iteration 14500 : Loss 5866.7533\n",
      "Iteration 14600 : Loss 5863.2961\n",
      "Iteration 14700 : Loss 5859.9466\n",
      "Iteration 14800 : Loss 5856.7006\n",
      "Iteration 14900 : Loss 5853.5539\n",
      "Iteration 15000 : Loss 5850.5028\n",
      "Iteration 15100 : Loss 5847.5434\n",
      "Iteration 15200 : Loss 5844.6723\n",
      "Iteration 15300 : Loss 5841.8858\n",
      "Iteration 15400 : Loss 5839.1808\n",
      "Iteration 15500 : Loss 5836.5541\n",
      "Iteration 15600 : Loss 5834.0025\n",
      "Iteration 15700 : Loss 5831.5231\n",
      "Iteration 15800 : Loss 5829.1132\n",
      "Iteration 15900 : Loss 5826.7699\n",
      "Iteration 16000 : Loss 5824.4908\n",
      "Iteration 16100 : Loss 5822.2732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16200 : Loss 5820.1149\n",
      "Iteration 16300 : Loss 5818.0134\n",
      "Iteration 16400 : Loss 5815.9665\n",
      "Iteration 16500 : Loss 5813.9722\n",
      "Iteration 16600 : Loss 5812.0283\n",
      "Iteration 16700 : Loss 5810.1330\n",
      "Iteration 16800 : Loss 5808.2842\n",
      "Iteration 16900 : Loss 5806.4802\n",
      "Iteration 17000 : Loss 5804.7193\n",
      "Iteration 17100 : Loss 5802.9997\n",
      "Iteration 17200 : Loss 5801.3198\n",
      "Iteration 17300 : Loss 5799.6782\n",
      "Iteration 17400 : Loss 5798.0732\n",
      "Iteration 17500 : Loss 5796.5036\n",
      "Iteration 17600 : Loss 5794.9678\n",
      "Iteration 17700 : Loss 5793.4646\n",
      "Iteration 17800 : Loss 5791.9927\n",
      "Iteration 17900 : Loss 5790.5509\n",
      "Iteration 18000 : Loss 5789.1381\n",
      "Iteration 18100 : Loss 5787.7530\n",
      "Iteration 18200 : Loss 5786.3946\n",
      "Iteration 18300 : Loss 5785.0620\n",
      "Iteration 18400 : Loss 5783.7540\n",
      "Iteration 18500 : Loss 5782.4697\n",
      "Iteration 18600 : Loss 5781.2082\n",
      "Iteration 18700 : Loss 5779.9686\n",
      "Iteration 18800 : Loss 5778.7501\n",
      "Iteration 18900 : Loss 5777.5518\n",
      "Iteration 19000 : Loss 5776.3730\n",
      "Iteration 19100 : Loss 5775.2128\n",
      "Iteration 19200 : Loss 5774.0707\n",
      "Iteration 19300 : Loss 5772.9459\n",
      "Iteration 19400 : Loss 5771.8376\n",
      "Iteration 19500 : Loss 5770.7453\n",
      "Iteration 19600 : Loss 5769.6684\n",
      "Iteration 19700 : Loss 5768.6063\n",
      "Iteration 19800 : Loss 5767.5583\n",
      "Iteration 19900 : Loss 5766.5240\n",
      "Iteration 20000 : Loss 5765.5028\n",
      "Iteration 20100 : Loss 5764.4942\n",
      "Iteration 20200 : Loss 5763.4977\n",
      "Iteration 20300 : Loss 5762.5129\n",
      "Iteration 20400 : Loss 5761.5393\n",
      "Iteration 20500 : Loss 5760.5765\n",
      "Iteration 20600 : Loss 5759.6240\n",
      "Iteration 20700 : Loss 5758.6815\n",
      "Iteration 20800 : Loss 5757.7486\n",
      "Iteration 20900 : Loss 5756.8249\n",
      "Iteration 21000 : Loss 5755.9101\n",
      "Iteration 21100 : Loss 5755.0037\n",
      "Iteration 21200 : Loss 5754.1056\n",
      "Iteration 21300 : Loss 5753.2154\n",
      "Iteration 21400 : Loss 5752.3328\n",
      "Iteration 21500 : Loss 5751.4574\n",
      "Iteration 21600 : Loss 5750.5891\n",
      "Iteration 21700 : Loss 5749.7276\n",
      "Iteration 21800 : Loss 5748.8725\n",
      "Iteration 21900 : Loss 5748.0237\n",
      "Iteration 22000 : Loss 5747.1809\n",
      "Iteration 22100 : Loss 5746.3439\n",
      "Iteration 22200 : Loss 5745.5124\n",
      "Iteration 22300 : Loss 5744.6863\n",
      "Iteration 22400 : Loss 5743.8654\n",
      "Iteration 22500 : Loss 5743.0494\n",
      "Iteration 22600 : Loss 5742.2382\n",
      "Iteration 22700 : Loss 5741.4316\n",
      "Iteration 22800 : Loss 5740.6295\n",
      "Iteration 22900 : Loss 5739.8316\n",
      "Iteration 23000 : Loss 5739.0378\n",
      "Iteration 23100 : Loss 5738.2479\n",
      "Iteration 23200 : Loss 5737.4619\n",
      "Iteration 23300 : Loss 5736.6795\n",
      "Iteration 23400 : Loss 5735.9007\n",
      "Iteration 23500 : Loss 5735.1252\n",
      "Iteration 23600 : Loss 5734.3531\n",
      "Iteration 23700 : Loss 5733.5840\n",
      "Iteration 23800 : Loss 5732.8181\n",
      "Iteration 23900 : Loss 5732.0550\n",
      "Iteration 24000 : Loss 5731.2948\n",
      "Iteration 24100 : Loss 5730.5373\n",
      "Iteration 24200 : Loss 5729.7824\n",
      "Iteration 24300 : Loss 5729.0300\n",
      "Iteration 24400 : Loss 5728.2801\n",
      "Iteration 24500 : Loss 5727.5326\n",
      "Iteration 24600 : Loss 5726.7873\n",
      "Iteration 24700 : Loss 5726.0441\n",
      "Iteration 24800 : Loss 5725.3031\n",
      "Iteration 24900 : Loss 5724.5642\n",
      "Iteration 25000 : Loss 5723.8272\n",
      "Iteration 25100 : Loss 5723.0921\n",
      "Iteration 25200 : Loss 5722.3588\n",
      "Iteration 25300 : Loss 5721.6273\n",
      "Iteration 25400 : Loss 5720.8975\n",
      "Iteration 25500 : Loss 5720.1693\n",
      "Iteration 25600 : Loss 5719.4427\n",
      "Iteration 25700 : Loss 5718.7177\n",
      "Iteration 25800 : Loss 5717.9941\n",
      "Iteration 25900 : Loss 5717.2720\n",
      "Iteration 26000 : Loss 5716.5513\n",
      "Iteration 26100 : Loss 5715.8319\n",
      "Iteration 26200 : Loss 5715.1138\n",
      "Iteration 26300 : Loss 5714.3970\n",
      "Iteration 26400 : Loss 5713.6814\n",
      "Iteration 26500 : Loss 5712.9670\n",
      "Iteration 26600 : Loss 5712.2537\n",
      "Iteration 26700 : Loss 5711.5415\n",
      "Iteration 26800 : Loss 5710.8304\n",
      "Iteration 26900 : Loss 5710.1203\n",
      "Iteration 27000 : Loss 5709.4112\n",
      "Iteration 27100 : Loss 5708.7030\n",
      "Iteration 27200 : Loss 5707.9959\n",
      "Iteration 27300 : Loss 5707.2896\n",
      "Iteration 27400 : Loss 5706.5842\n",
      "Iteration 27500 : Loss 5705.8797\n",
      "Iteration 27600 : Loss 5705.1760\n",
      "Iteration 27700 : Loss 5704.4731\n",
      "Iteration 27800 : Loss 5703.7710\n",
      "Iteration 27900 : Loss 5703.0697\n",
      "Iteration 28000 : Loss 5702.3691\n",
      "Iteration 28100 : Loss 5701.6693\n",
      "Iteration 28200 : Loss 5700.9701\n",
      "Iteration 28300 : Loss 5700.2716\n",
      "Iteration 28400 : Loss 5699.5738\n",
      "Iteration 28500 : Loss 5698.8766\n",
      "Iteration 28600 : Loss 5698.1801\n",
      "Iteration 28700 : Loss 5697.4842\n",
      "Iteration 28800 : Loss 5696.7889\n",
      "Iteration 28900 : Loss 5696.0941\n",
      "Iteration 29000 : Loss 5695.3999\n",
      "Iteration 29100 : Loss 5694.7063\n",
      "Iteration 29200 : Loss 5694.0132\n",
      "Iteration 29300 : Loss 5693.3207\n",
      "Iteration 29400 : Loss 5692.6287\n",
      "Iteration 29500 : Loss 5691.9371\n",
      "Iteration 29600 : Loss 5691.2461\n",
      "Iteration 29700 : Loss 5690.5556\n",
      "Iteration 29800 : Loss 5689.8655\n",
      "Iteration 29900 : Loss 5689.1759\n",
      "Iteration 30000 : Loss 5688.4867\n",
      "====================================\n",
      "학습률: 0.01, 총 epoch: 1001\n",
      "Iteration 100 : Loss 6027.0578\n",
      "Iteration 200 : Loss 5562.9380\n",
      "Iteration 300 : Loss 5492.7533\n",
      "Iteration 400 : Loss 5431.4107\n",
      "Iteration 500 : Loss 5372.0737\n",
      "Iteration 600 : Loss 5314.5605\n",
      "Iteration 700 : Loss 5258.8084\n",
      "Iteration 800 : Loss 5204.7588\n",
      "Iteration 900 : Loss 5152.3550\n",
      "Iteration 1000 : Loss 5101.5424\n",
      "====================================\n",
      "학습률: 0.01, 총 epoch: 10001\n",
      "Iteration 100 : Loss 5458.1751\n",
      "Iteration 200 : Loss 5011.4978\n",
      "Iteration 300 : Loss 4958.1304\n",
      "Iteration 400 : Loss 4913.0458\n",
      "Iteration 500 : Loss 4869.4273\n",
      "Iteration 600 : Loss 4827.1115\n",
      "Iteration 700 : Loss 4786.0534\n",
      "Iteration 800 : Loss 4746.2116\n",
      "Iteration 900 : Loss 4707.5460\n",
      "Iteration 1000 : Loss 4670.0181\n",
      "Iteration 1100 : Loss 4633.5904\n",
      "Iteration 1200 : Loss 4598.2269\n",
      "Iteration 1300 : Loss 4563.8927\n",
      "Iteration 1400 : Loss 4530.5542\n",
      "Iteration 1500 : Loss 4498.1789\n",
      "Iteration 1600 : Loss 4466.7354\n",
      "Iteration 1700 : Loss 4436.1932\n",
      "Iteration 1800 : Loss 4406.5231\n",
      "Iteration 1900 : Loss 4377.6968\n",
      "Iteration 2000 : Loss 4349.6868\n",
      "Iteration 2100 : Loss 4322.4668\n",
      "Iteration 2200 : Loss 4296.0112\n",
      "Iteration 2300 : Loss 4270.2953\n",
      "Iteration 2400 : Loss 4245.2952\n",
      "Iteration 2500 : Loss 4220.9879\n",
      "Iteration 2600 : Loss 4197.3512\n",
      "Iteration 2700 : Loss 4174.3635\n",
      "Iteration 2800 : Loss 4152.0041\n",
      "Iteration 2900 : Loss 4130.2528\n",
      "Iteration 3000 : Loss 4109.0903\n",
      "Iteration 3100 : Loss 4088.4977\n",
      "Iteration 3200 : Loss 4068.4570\n",
      "Iteration 3300 : Loss 4048.9507\n",
      "Iteration 3400 : Loss 4029.9618\n",
      "Iteration 3500 : Loss 4011.4739\n",
      "Iteration 3600 : Loss 3993.4713\n",
      "Iteration 3700 : Loss 3975.9386\n",
      "Iteration 3800 : Loss 3958.8612\n",
      "Iteration 3900 : Loss 3942.2247\n",
      "Iteration 4000 : Loss 3926.0153\n",
      "Iteration 4100 : Loss 3910.2197\n",
      "Iteration 4200 : Loss 3894.8251\n",
      "Iteration 4300 : Loss 3879.8189\n",
      "Iteration 4400 : Loss 3865.1891\n",
      "Iteration 4500 : Loss 3850.9242\n",
      "Iteration 4600 : Loss 3837.0127\n",
      "Iteration 4700 : Loss 3823.4440\n",
      "Iteration 4800 : Loss 3810.2073\n",
      "Iteration 4900 : Loss 3797.2927\n",
      "Iteration 5000 : Loss 3784.6902\n",
      "Iteration 5100 : Loss 3772.3903\n",
      "Iteration 5200 : Loss 3760.3840\n",
      "Iteration 5300 : Loss 3748.6622\n",
      "Iteration 5400 : Loss 3737.2164\n",
      "Iteration 5500 : Loss 3726.0383\n",
      "Iteration 5600 : Loss 3715.1198\n",
      "Iteration 5700 : Loss 3704.4533\n",
      "Iteration 5800 : Loss 3694.0311\n",
      "Iteration 5900 : Loss 3683.8461\n",
      "Iteration 6000 : Loss 3673.8912\n",
      "Iteration 6100 : Loss 3664.1596\n",
      "Iteration 6200 : Loss 3654.6447\n",
      "Iteration 6300 : Loss 3645.3403\n",
      "Iteration 6400 : Loss 3636.2400\n",
      "Iteration 6500 : Loss 3627.3381\n",
      "Iteration 6600 : Loss 3618.6287\n",
      "Iteration 6700 : Loss 3610.1064\n",
      "Iteration 6800 : Loss 3601.7656\n",
      "Iteration 6900 : Loss 3593.6012\n",
      "Iteration 7000 : Loss 3585.6082\n",
      "Iteration 7100 : Loss 3577.7816\n",
      "Iteration 7200 : Loss 3570.1169\n",
      "Iteration 7300 : Loss 3562.6093\n",
      "Iteration 7400 : Loss 3555.2545\n",
      "Iteration 7500 : Loss 3548.0482\n",
      "Iteration 7600 : Loss 3540.9863\n",
      "Iteration 7700 : Loss 3534.0648\n",
      "Iteration 7800 : Loss 3527.2797\n",
      "Iteration 7900 : Loss 3520.6274\n",
      "Iteration 8000 : Loss 3514.1042\n",
      "Iteration 8100 : Loss 3507.7066\n",
      "Iteration 8200 : Loss 3501.4312\n",
      "Iteration 8300 : Loss 3495.2747\n",
      "Iteration 8400 : Loss 3489.2339\n",
      "Iteration 8500 : Loss 3483.3057\n",
      "Iteration 8600 : Loss 3477.4871\n",
      "Iteration 8700 : Loss 3471.7753\n",
      "Iteration 8800 : Loss 3466.1673\n",
      "Iteration 8900 : Loss 3460.6606\n",
      "Iteration 9000 : Loss 3455.2524\n",
      "Iteration 9100 : Loss 3449.9402\n",
      "Iteration 9200 : Loss 3444.7216\n",
      "Iteration 9300 : Loss 3439.5941\n",
      "Iteration 9400 : Loss 3434.5555\n",
      "Iteration 9500 : Loss 3429.6034\n",
      "Iteration 9600 : Loss 3424.7358\n",
      "Iteration 9700 : Loss 3419.9504\n",
      "Iteration 9800 : Loss 3415.2452\n",
      "Iteration 9900 : Loss 3410.6184\n",
      "Iteration 10000 : Loss 3406.0678\n",
      "====================================\n",
      "학습률: 0.01, 총 epoch: 30001\n",
      "Iteration 100 : Loss 3806.9170\n",
      "Iteration 200 : Loss 3404.1728\n",
      "Iteration 300 : Loss 3392.8359\n",
      "Iteration 400 : Loss 3388.4506\n",
      "Iteration 500 : Loss 3384.2539\n",
      "Iteration 600 : Loss 3380.1253\n",
      "Iteration 700 : Loss 3376.0612\n",
      "Iteration 800 : Loss 3372.0601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 900 : Loss 3368.1204\n",
      "Iteration 1000 : Loss 3364.2407\n",
      "Iteration 1100 : Loss 3360.4197\n",
      "Iteration 1200 : Loss 3356.6561\n",
      "Iteration 1300 : Loss 3352.9485\n",
      "Iteration 1400 : Loss 3349.2957\n",
      "Iteration 1500 : Loss 3345.6965\n",
      "Iteration 1600 : Loss 3342.1496\n",
      "Iteration 1700 : Loss 3338.6540\n",
      "Iteration 1800 : Loss 3335.2085\n",
      "Iteration 1900 : Loss 3331.8120\n",
      "Iteration 2000 : Loss 3328.4635\n",
      "Iteration 2100 : Loss 3325.1620\n",
      "Iteration 2200 : Loss 3321.9064\n",
      "Iteration 2300 : Loss 3318.6958\n",
      "Iteration 2400 : Loss 3315.5293\n",
      "Iteration 2500 : Loss 3312.4059\n",
      "Iteration 2600 : Loss 3309.3247\n",
      "Iteration 2700 : Loss 3306.2850\n",
      "Iteration 2800 : Loss 3303.2858\n",
      "Iteration 2900 : Loss 3300.3264\n",
      "Iteration 3000 : Loss 3297.4059\n",
      "Iteration 3100 : Loss 3294.5236\n",
      "Iteration 3200 : Loss 3291.6787\n",
      "Iteration 3300 : Loss 3288.8705\n",
      "Iteration 3400 : Loss 3286.0983\n",
      "Iteration 3500 : Loss 3283.3614\n",
      "Iteration 3600 : Loss 3280.6591\n",
      "Iteration 3700 : Loss 3277.9909\n",
      "Iteration 3800 : Loss 3275.3559\n",
      "Iteration 3900 : Loss 3272.7537\n",
      "Iteration 4000 : Loss 3270.1836\n",
      "Iteration 4100 : Loss 3267.6450\n",
      "Iteration 4200 : Loss 3265.1373\n",
      "Iteration 4300 : Loss 3262.6601\n",
      "Iteration 4400 : Loss 3260.2127\n",
      "Iteration 4500 : Loss 3257.7947\n",
      "Iteration 4600 : Loss 3255.4054\n",
      "Iteration 4700 : Loss 3253.0445\n",
      "Iteration 4800 : Loss 3250.7114\n",
      "Iteration 4900 : Loss 3248.4056\n",
      "Iteration 5000 : Loss 3246.1267\n",
      "Iteration 5100 : Loss 3243.8742\n",
      "Iteration 5200 : Loss 3241.6476\n",
      "Iteration 5300 : Loss 3239.4467\n",
      "Iteration 5400 : Loss 3237.2708\n",
      "Iteration 5500 : Loss 3235.1196\n",
      "Iteration 5600 : Loss 3232.9928\n",
      "Iteration 5700 : Loss 3230.8899\n",
      "Iteration 5800 : Loss 3228.8105\n",
      "Iteration 5900 : Loss 3226.7542\n",
      "Iteration 6000 : Loss 3224.7208\n",
      "Iteration 6100 : Loss 3222.7097\n",
      "Iteration 6200 : Loss 3220.7208\n",
      "Iteration 6300 : Loss 3218.7536\n",
      "Iteration 6400 : Loss 3216.8078\n",
      "Iteration 6500 : Loss 3214.8831\n",
      "Iteration 6600 : Loss 3212.9792\n",
      "Iteration 6700 : Loss 3211.0957\n",
      "Iteration 6800 : Loss 3209.2324\n",
      "Iteration 6900 : Loss 3207.3890\n",
      "Iteration 7000 : Loss 3205.5651\n",
      "Iteration 7100 : Loss 3203.7604\n",
      "Iteration 7200 : Loss 3201.9748\n",
      "Iteration 7300 : Loss 3200.2080\n",
      "Iteration 7400 : Loss 3198.4595\n",
      "Iteration 7500 : Loss 3196.7293\n",
      "Iteration 7600 : Loss 3195.0170\n",
      "Iteration 7700 : Loss 3193.3224\n",
      "Iteration 7800 : Loss 3191.6453\n",
      "Iteration 7900 : Loss 3189.9854\n",
      "Iteration 8000 : Loss 3188.3424\n",
      "Iteration 8100 : Loss 3186.7162\n",
      "Iteration 8200 : Loss 3185.1066\n",
      "Iteration 8300 : Loss 3183.5132\n",
      "Iteration 8400 : Loss 3181.9359\n",
      "Iteration 8500 : Loss 3180.3744\n",
      "Iteration 8600 : Loss 3178.8286\n",
      "Iteration 8700 : Loss 3177.2983\n",
      "Iteration 8800 : Loss 3175.7832\n",
      "Iteration 8900 : Loss 3174.2832\n",
      "Iteration 9000 : Loss 3172.7980\n",
      "Iteration 9100 : Loss 3171.3275\n",
      "Iteration 9200 : Loss 3169.8715\n",
      "Iteration 9300 : Loss 3168.4298\n",
      "Iteration 9400 : Loss 3167.0023\n",
      "Iteration 9500 : Loss 3165.5886\n",
      "Iteration 9600 : Loss 3164.1888\n",
      "Iteration 9700 : Loss 3162.8025\n",
      "Iteration 9800 : Loss 3161.4297\n",
      "Iteration 9900 : Loss 3160.0702\n",
      "Iteration 10000 : Loss 3158.7238\n",
      "Iteration 10100 : Loss 3157.3903\n",
      "Iteration 10200 : Loss 3156.0697\n",
      "Iteration 10300 : Loss 3154.7617\n",
      "Iteration 10400 : Loss 3153.4661\n",
      "Iteration 10500 : Loss 3152.1830\n",
      "Iteration 10600 : Loss 3150.9120\n",
      "Iteration 10700 : Loss 3149.6531\n",
      "Iteration 10800 : Loss 3148.4061\n",
      "Iteration 10900 : Loss 3147.1709\n",
      "Iteration 11000 : Loss 3145.9474\n",
      "Iteration 11100 : Loss 3144.7353\n",
      "Iteration 11200 : Loss 3143.5347\n",
      "Iteration 11300 : Loss 3142.3452\n",
      "Iteration 11400 : Loss 3141.1670\n",
      "Iteration 11500 : Loss 3139.9997\n",
      "Iteration 11600 : Loss 3138.8433\n",
      "Iteration 11700 : Loss 3137.6976\n",
      "Iteration 11800 : Loss 3136.5626\n",
      "Iteration 11900 : Loss 3135.4381\n",
      "Iteration 12000 : Loss 3134.3239\n",
      "Iteration 12100 : Loss 3133.2201\n",
      "Iteration 12200 : Loss 3132.1265\n",
      "Iteration 12300 : Loss 3131.0429\n",
      "Iteration 12400 : Loss 3129.9692\n",
      "Iteration 12500 : Loss 3128.9054\n",
      "Iteration 12600 : Loss 3127.8513\n",
      "Iteration 12700 : Loss 3126.8069\n",
      "Iteration 12800 : Loss 3125.7720\n",
      "Iteration 12900 : Loss 3124.7465\n",
      "Iteration 13000 : Loss 3123.7303\n",
      "Iteration 13100 : Loss 3122.7234\n",
      "Iteration 13200 : Loss 3121.7256\n",
      "Iteration 13300 : Loss 3120.7368\n",
      "Iteration 13400 : Loss 3119.7570\n",
      "Iteration 13500 : Loss 3118.7860\n",
      "Iteration 13600 : Loss 3117.8238\n",
      "Iteration 13700 : Loss 3116.8702\n",
      "Iteration 13800 : Loss 3115.9252\n",
      "Iteration 13900 : Loss 3114.9887\n",
      "Iteration 14000 : Loss 3114.0606\n",
      "Iteration 14100 : Loss 3113.1407\n",
      "Iteration 14200 : Loss 3112.2291\n",
      "Iteration 14300 : Loss 3111.3257\n",
      "Iteration 14400 : Loss 3110.4303\n",
      "Iteration 14500 : Loss 3109.5428\n",
      "Iteration 14600 : Loss 3108.6633\n",
      "Iteration 14700 : Loss 3107.7916\n",
      "Iteration 14800 : Loss 3106.9276\n",
      "Iteration 14900 : Loss 3106.0712\n",
      "Iteration 15000 : Loss 3105.2225\n",
      "Iteration 15100 : Loss 3104.3812\n",
      "Iteration 15200 : Loss 3103.5474\n",
      "Iteration 15300 : Loss 3102.7209\n",
      "Iteration 15400 : Loss 3101.9017\n",
      "Iteration 15500 : Loss 3101.0897\n",
      "Iteration 15600 : Loss 3100.2849\n",
      "Iteration 15700 : Loss 3099.4871\n",
      "Iteration 15800 : Loss 3098.6963\n",
      "Iteration 15900 : Loss 3097.9124\n",
      "Iteration 16000 : Loss 3097.1354\n",
      "Iteration 16100 : Loss 3096.3652\n",
      "Iteration 16200 : Loss 3095.6017\n",
      "Iteration 16300 : Loss 3094.8449\n",
      "Iteration 16400 : Loss 3094.0946\n",
      "Iteration 16500 : Loss 3093.3509\n",
      "Iteration 16600 : Loss 3092.6137\n",
      "Iteration 16700 : Loss 3091.8828\n",
      "Iteration 16800 : Loss 3091.1583\n",
      "Iteration 16900 : Loss 3090.4401\n",
      "Iteration 17000 : Loss 3089.7281\n",
      "Iteration 17100 : Loss 3089.0223\n",
      "Iteration 17200 : Loss 3088.3225\n",
      "Iteration 17300 : Loss 3087.6289\n",
      "Iteration 17400 : Loss 3086.9411\n",
      "Iteration 17500 : Loss 3086.2594\n",
      "Iteration 17600 : Loss 3085.5834\n",
      "Iteration 17700 : Loss 3084.9134\n",
      "Iteration 17800 : Loss 3084.2490\n",
      "Iteration 17900 : Loss 3083.5904\n",
      "Iteration 18000 : Loss 3082.9374\n",
      "Iteration 18100 : Loss 3082.2900\n",
      "Iteration 18200 : Loss 3081.6482\n",
      "Iteration 18300 : Loss 3081.0118\n",
      "Iteration 18400 : Loss 3080.3809\n",
      "Iteration 18500 : Loss 3079.7554\n",
      "Iteration 18600 : Loss 3079.1352\n",
      "Iteration 18700 : Loss 3078.5203\n",
      "Iteration 18800 : Loss 3077.9106\n",
      "Iteration 18900 : Loss 3077.3062\n",
      "Iteration 19000 : Loss 3076.7068\n",
      "Iteration 19100 : Loss 3076.1126\n",
      "Iteration 19200 : Loss 3075.5234\n",
      "Iteration 19300 : Loss 3074.9392\n",
      "Iteration 19400 : Loss 3074.3599\n",
      "Iteration 19500 : Loss 3073.7856\n",
      "Iteration 19600 : Loss 3073.2161\n",
      "Iteration 19700 : Loss 3072.6515\n",
      "Iteration 19800 : Loss 3072.0916\n",
      "Iteration 19900 : Loss 3071.5364\n",
      "Iteration 20000 : Loss 3070.9859\n",
      "Iteration 20100 : Loss 3070.4401\n",
      "Iteration 20200 : Loss 3069.8988\n",
      "Iteration 20300 : Loss 3069.3621\n",
      "Iteration 20400 : Loss 3068.8299\n",
      "Iteration 20500 : Loss 3068.3022\n",
      "Iteration 20600 : Loss 3067.7789\n",
      "Iteration 20700 : Loss 3067.2600\n",
      "Iteration 20800 : Loss 3066.7455\n",
      "Iteration 20900 : Loss 3066.2352\n",
      "Iteration 21000 : Loss 3065.7292\n",
      "Iteration 21100 : Loss 3065.2275\n",
      "Iteration 21200 : Loss 3064.7299\n",
      "Iteration 21300 : Loss 3064.2365\n",
      "Iteration 21400 : Loss 3063.7473\n",
      "Iteration 21500 : Loss 3063.2621\n",
      "Iteration 21600 : Loss 3062.7809\n",
      "Iteration 21700 : Loss 3062.3037\n",
      "Iteration 21800 : Loss 3061.8305\n",
      "Iteration 21900 : Loss 3061.3613\n",
      "Iteration 22000 : Loss 3060.8959\n",
      "Iteration 22100 : Loss 3060.4344\n",
      "Iteration 22200 : Loss 3059.9767\n",
      "Iteration 22300 : Loss 3059.5228\n",
      "Iteration 22400 : Loss 3059.0727\n",
      "Iteration 22500 : Loss 3058.6263\n",
      "Iteration 22600 : Loss 3058.1836\n",
      "Iteration 22700 : Loss 3057.7445\n",
      "Iteration 22800 : Loss 3057.3091\n",
      "Iteration 22900 : Loss 3056.8773\n",
      "Iteration 23000 : Loss 3056.4490\n",
      "Iteration 23100 : Loss 3056.0242\n",
      "Iteration 23200 : Loss 3055.6030\n",
      "Iteration 23300 : Loss 3055.1852\n",
      "Iteration 23400 : Loss 3054.7708\n",
      "Iteration 23500 : Loss 3054.3599\n",
      "Iteration 23600 : Loss 3053.9523\n",
      "Iteration 23700 : Loss 3053.5480\n",
      "Iteration 23800 : Loss 3053.1471\n",
      "Iteration 23900 : Loss 3052.7494\n",
      "Iteration 24000 : Loss 3052.3551\n",
      "Iteration 24100 : Loss 3051.9639\n",
      "Iteration 24200 : Loss 3051.5759\n",
      "Iteration 24300 : Loss 3051.1911\n",
      "Iteration 24400 : Loss 3050.8094\n",
      "Iteration 24500 : Loss 3050.4309\n",
      "Iteration 24600 : Loss 3050.0554\n",
      "Iteration 24700 : Loss 3049.6830\n",
      "Iteration 24800 : Loss 3049.3136\n",
      "Iteration 24900 : Loss 3048.9472\n",
      "Iteration 25000 : Loss 3048.5838\n",
      "Iteration 25100 : Loss 3048.2233\n",
      "Iteration 25200 : Loss 3047.8658\n",
      "Iteration 25300 : Loss 3047.5112\n",
      "Iteration 25400 : Loss 3047.1594\n",
      "Iteration 25500 : Loss 3046.8105\n",
      "Iteration 25600 : Loss 3046.4644\n",
      "Iteration 25700 : Loss 3046.1211\n",
      "Iteration 25800 : Loss 3045.7806\n",
      "Iteration 25900 : Loss 3045.4428\n",
      "Iteration 26000 : Loss 3045.1077\n",
      "Iteration 26100 : Loss 3044.7754\n",
      "Iteration 26200 : Loss 3044.4457\n",
      "Iteration 26300 : Loss 3044.1187\n",
      "Iteration 26400 : Loss 3043.7943\n",
      "Iteration 26500 : Loss 3043.4725\n",
      "Iteration 26600 : Loss 3043.1533\n",
      "Iteration 26700 : Loss 3042.8366\n",
      "Iteration 26800 : Loss 3042.5225\n",
      "Iteration 26900 : Loss 3042.2110\n",
      "Iteration 27000 : Loss 3041.9019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 27100 : Loss 3041.5952\n",
      "Iteration 27200 : Loss 3041.2911\n",
      "Iteration 27300 : Loss 3040.9893\n",
      "Iteration 27400 : Loss 3040.6900\n",
      "Iteration 27500 : Loss 3040.3931\n",
      "Iteration 27600 : Loss 3040.0985\n",
      "Iteration 27700 : Loss 3039.8063\n",
      "Iteration 27800 : Loss 3039.5164\n",
      "Iteration 27900 : Loss 3039.2288\n",
      "Iteration 28000 : Loss 3038.9435\n",
      "Iteration 28100 : Loss 3038.6605\n",
      "Iteration 28200 : Loss 3038.3797\n",
      "Iteration 28300 : Loss 3038.1011\n",
      "Iteration 28400 : Loss 3037.8248\n",
      "Iteration 28500 : Loss 3037.5506\n",
      "Iteration 28600 : Loss 3037.2786\n",
      "Iteration 28700 : Loss 3037.0088\n",
      "Iteration 28800 : Loss 3036.7411\n",
      "Iteration 28900 : Loss 3036.4755\n",
      "Iteration 29000 : Loss 3036.2120\n",
      "Iteration 29100 : Loss 3035.9506\n",
      "Iteration 29200 : Loss 3035.6912\n",
      "Iteration 29300 : Loss 3035.4339\n",
      "Iteration 29400 : Loss 3035.1786\n",
      "Iteration 29500 : Loss 3034.9253\n",
      "Iteration 29600 : Loss 3034.6740\n",
      "Iteration 29700 : Loss 3034.4247\n",
      "Iteration 29800 : Loss 3034.1774\n",
      "Iteration 29900 : Loss 3033.9319\n",
      "Iteration 30000 : Loss 3033.6884\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "# epoch: 1001, 10001\n",
    "EPOCH = [1001, 10001, 30001]\n",
    "\n",
    "# 총 여섯 가지 조합의 학습률과 에포크로 학습\n",
    "for lr in LEARNING_RATE:\n",
    "    for epoch in EPOCH:\n",
    "        print(\"====================================\")\n",
    "        print(f\"학습률: {lr}, 총 epoch: {epoch}\")\n",
    "        W_train = W\n",
    "        b_train = b\n",
    "        for i in range(1, epoch):\n",
    "            dW, db = gradient(X_train, W_train, b_train, y_train)\n",
    "            W_train -= lr * dW\n",
    "            b_train -= lr * db\n",
    "            L = loss(X_train, W_train, b_train, y_train)\n",
    "            if i % 100 == 0:\n",
    "                print('Iteration %d : Loss %0.4f' % (i, L))\n",
    "        losses.append(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4fd81250",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21457.973336389095, 6326.192461138382, 5688.486686313191, 5101.542429363352, 3406.0678408181425, 3033.688448713556]\n"
     ]
    }
   ],
   "source": [
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7036d746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100 : Loss 3438.1729\n",
      "Iteration 200 : Loss 3040.3111\n",
      "Iteration 300 : Loss 3033.0807\n",
      "Iteration 400 : Loss 3032.7220\n",
      "Iteration 500 : Loss 3032.4861\n",
      "Iteration 600 : Loss 3032.2541\n",
      "Iteration 700 : Loss 3032.0239\n",
      "Iteration 800 : Loss 3031.7955\n",
      "Iteration 900 : Loss 3031.5689\n",
      "Iteration 1000 : Loss 3031.3441\n",
      "Iteration 1100 : Loss 3031.1210\n",
      "Iteration 1200 : Loss 3030.8996\n",
      "Iteration 1300 : Loss 3030.6800\n",
      "Iteration 1400 : Loss 3030.4620\n",
      "Iteration 1500 : Loss 3030.2458\n",
      "Iteration 1600 : Loss 3030.0312\n",
      "Iteration 1700 : Loss 3029.8183\n",
      "Iteration 1800 : Loss 3029.6070\n",
      "Iteration 1900 : Loss 3029.3974\n",
      "Iteration 2000 : Loss 3029.1893\n",
      "Iteration 2100 : Loss 3028.9829\n",
      "Iteration 2200 : Loss 3028.7781\n",
      "Iteration 2300 : Loss 3028.5748\n",
      "Iteration 2400 : Loss 3028.3731\n",
      "Iteration 2500 : Loss 3028.1730\n",
      "Iteration 2600 : Loss 3027.9744\n",
      "Iteration 2700 : Loss 3027.7773\n",
      "Iteration 2800 : Loss 3027.5817\n",
      "Iteration 2900 : Loss 3027.3877\n",
      "Iteration 3000 : Loss 3027.1951\n",
      "Iteration 3100 : Loss 3027.0040\n",
      "Iteration 3200 : Loss 3026.8144\n",
      "Iteration 3300 : Loss 3026.6262\n",
      "Iteration 3400 : Loss 3026.4394\n",
      "Iteration 3500 : Loss 3026.2541\n",
      "Iteration 3600 : Loss 3026.0702\n",
      "Iteration 3700 : Loss 3025.8876\n",
      "Iteration 3800 : Loss 3025.7065\n",
      "Iteration 3900 : Loss 3025.5268\n",
      "Iteration 4000 : Loss 3025.3484\n",
      "Iteration 4100 : Loss 3025.1714\n",
      "Iteration 4200 : Loss 3024.9957\n",
      "Iteration 4300 : Loss 3024.8213\n",
      "Iteration 4400 : Loss 3024.6483\n",
      "Iteration 4500 : Loss 3024.4766\n",
      "Iteration 4600 : Loss 3024.3062\n",
      "Iteration 4700 : Loss 3024.1370\n",
      "Iteration 4800 : Loss 3023.9692\n",
      "Iteration 4900 : Loss 3023.8026\n",
      "Iteration 5000 : Loss 3023.6373\n",
      "Iteration 5100 : Loss 3023.4732\n",
      "Iteration 5200 : Loss 3023.3104\n",
      "Iteration 5300 : Loss 3023.1488\n",
      "Iteration 5400 : Loss 3022.9884\n",
      "Iteration 5500 : Loss 3022.8292\n",
      "Iteration 5600 : Loss 3022.6712\n",
      "Iteration 5700 : Loss 3022.5144\n",
      "Iteration 5800 : Loss 3022.3588\n",
      "Iteration 5900 : Loss 3022.2043\n",
      "Iteration 6000 : Loss 3022.0510\n",
      "Iteration 6100 : Loss 3021.8988\n",
      "Iteration 6200 : Loss 3021.7478\n",
      "Iteration 6300 : Loss 3021.5979\n",
      "Iteration 6400 : Loss 3021.4492\n",
      "Iteration 6500 : Loss 3021.3015\n",
      "Iteration 6600 : Loss 3021.1549\n",
      "Iteration 6700 : Loss 3021.0095\n",
      "Iteration 6800 : Loss 3020.8651\n",
      "Iteration 6900 : Loss 3020.7218\n",
      "Iteration 7000 : Loss 3020.5795\n",
      "Iteration 7100 : Loss 3020.4384\n",
      "Iteration 7200 : Loss 3020.2982\n",
      "Iteration 7300 : Loss 3020.1591\n",
      "Iteration 7400 : Loss 3020.0211\n",
      "Iteration 7500 : Loss 3019.8840\n",
      "Iteration 7600 : Loss 3019.7480\n",
      "Iteration 7700 : Loss 3019.6130\n",
      "Iteration 7800 : Loss 3019.4789\n",
      "Iteration 7900 : Loss 3019.3459\n",
      "Iteration 8000 : Loss 3019.2139\n",
      "Iteration 8100 : Loss 3019.0828\n",
      "Iteration 8200 : Loss 3018.9527\n",
      "Iteration 8300 : Loss 3018.8235\n",
      "Iteration 8400 : Loss 3018.6953\n",
      "Iteration 8500 : Loss 3018.5680\n",
      "Iteration 8600 : Loss 3018.4417\n",
      "Iteration 8700 : Loss 3018.3163\n",
      "Iteration 8800 : Loss 3018.1918\n",
      "Iteration 8900 : Loss 3018.0683\n",
      "Iteration 9000 : Loss 3017.9456\n",
      "Iteration 9100 : Loss 3017.8238\n",
      "Iteration 9200 : Loss 3017.7030\n",
      "Iteration 9300 : Loss 3017.5830\n",
      "Iteration 9400 : Loss 3017.4639\n",
      "Iteration 9500 : Loss 3017.3456\n",
      "Iteration 9600 : Loss 3017.2282\n",
      "Iteration 9700 : Loss 3017.1117\n",
      "Iteration 9800 : Loss 3016.9960\n",
      "Iteration 9900 : Loss 3016.8812\n",
      "Iteration 10000 : Loss 3016.7672\n",
      "Iteration 10100 : Loss 3016.6540\n",
      "Iteration 10200 : Loss 3016.5417\n",
      "Iteration 10300 : Loss 3016.4301\n",
      "Iteration 10400 : Loss 3016.3194\n",
      "Iteration 10500 : Loss 3016.2095\n",
      "Iteration 10600 : Loss 3016.1003\n",
      "Iteration 10700 : Loss 3015.9920\n",
      "Iteration 10800 : Loss 3015.8844\n",
      "Iteration 10900 : Loss 3015.7776\n",
      "Iteration 11000 : Loss 3015.6716\n",
      "Iteration 11100 : Loss 3015.5664\n",
      "Iteration 11200 : Loss 3015.4619\n",
      "Iteration 11300 : Loss 3015.3581\n",
      "Iteration 11400 : Loss 3015.2551\n",
      "Iteration 11500 : Loss 3015.1529\n",
      "Iteration 11600 : Loss 3015.0513\n",
      "Iteration 11700 : Loss 3014.9506\n",
      "Iteration 11800 : Loss 3014.8505\n",
      "Iteration 11900 : Loss 3014.7511\n",
      "Iteration 12000 : Loss 3014.6525\n",
      "Iteration 12100 : Loss 3014.5545\n",
      "Iteration 12200 : Loss 3014.4573\n",
      "Iteration 12300 : Loss 3014.3608\n",
      "Iteration 12400 : Loss 3014.2649\n",
      "Iteration 12500 : Loss 3014.1697\n",
      "Iteration 12600 : Loss 3014.0752\n",
      "Iteration 12700 : Loss 3013.9814\n",
      "Iteration 12800 : Loss 3013.8883\n",
      "Iteration 12900 : Loss 3013.7958\n",
      "Iteration 13000 : Loss 3013.7039\n",
      "Iteration 13100 : Loss 3013.6127\n",
      "Iteration 13200 : Loss 3013.5222\n",
      "Iteration 13300 : Loss 3013.4323\n",
      "Iteration 13400 : Loss 3013.3430\n",
      "Iteration 13500 : Loss 3013.2544\n",
      "Iteration 13600 : Loss 3013.1664\n",
      "Iteration 13700 : Loss 3013.0790\n",
      "Iteration 13800 : Loss 3012.9923\n",
      "Iteration 13900 : Loss 3012.9061\n",
      "Iteration 14000 : Loss 3012.8205\n",
      "Iteration 14100 : Loss 3012.7356\n",
      "Iteration 14200 : Loss 3012.6513\n",
      "Iteration 14300 : Loss 3012.5675\n",
      "Iteration 14400 : Loss 3012.4843\n",
      "Iteration 14500 : Loss 3012.4017\n",
      "Iteration 14600 : Loss 3012.3197\n",
      "Iteration 14700 : Loss 3012.2383\n",
      "Iteration 14800 : Loss 3012.1574\n",
      "Iteration 14900 : Loss 3012.0772\n",
      "Iteration 15000 : Loss 3011.9974\n",
      "Iteration 15100 : Loss 3011.9182\n",
      "Iteration 15200 : Loss 3011.8396\n",
      "Iteration 15300 : Loss 3011.7615\n",
      "Iteration 15400 : Loss 3011.6840\n",
      "Iteration 15500 : Loss 3011.6070\n",
      "Iteration 15600 : Loss 3011.5305\n",
      "Iteration 15700 : Loss 3011.4546\n",
      "Iteration 15800 : Loss 3011.3792\n",
      "Iteration 15900 : Loss 3011.3043\n",
      "Iteration 16000 : Loss 3011.2300\n",
      "Iteration 16100 : Loss 3011.1561\n",
      "Iteration 16200 : Loss 3011.0828\n",
      "Iteration 16300 : Loss 3011.0100\n",
      "Iteration 16400 : Loss 3010.9377\n",
      "Iteration 16500 : Loss 3010.8658\n",
      "Iteration 16600 : Loss 3010.7945\n",
      "Iteration 16700 : Loss 3010.7237\n",
      "Iteration 16800 : Loss 3010.6533\n",
      "Iteration 16900 : Loss 3010.5835\n",
      "Iteration 17000 : Loss 3010.5141\n",
      "Iteration 17100 : Loss 3010.4452\n",
      "Iteration 17200 : Loss 3010.3767\n",
      "Iteration 17300 : Loss 3010.3088\n",
      "Iteration 17400 : Loss 3010.2413\n",
      "Iteration 17500 : Loss 3010.1742\n",
      "Iteration 17600 : Loss 3010.1077\n",
      "Iteration 17700 : Loss 3010.0415\n",
      "Iteration 17800 : Loss 3009.9759\n",
      "Iteration 17900 : Loss 3009.9107\n",
      "Iteration 18000 : Loss 3009.8459\n",
      "Iteration 18100 : Loss 3009.7815\n",
      "Iteration 18200 : Loss 3009.7176\n",
      "Iteration 18300 : Loss 3009.6542\n",
      "Iteration 18400 : Loss 3009.5911\n",
      "Iteration 18500 : Loss 3009.5285\n",
      "Iteration 18600 : Loss 3009.4664\n",
      "Iteration 18700 : Loss 3009.4046\n",
      "Iteration 18800 : Loss 3009.3433\n",
      "Iteration 18900 : Loss 3009.2823\n",
      "Iteration 19000 : Loss 3009.2218\n",
      "Iteration 19100 : Loss 3009.1617\n",
      "Iteration 19200 : Loss 3009.1020\n",
      "Iteration 19300 : Loss 3009.0427\n",
      "Iteration 19400 : Loss 3008.9838\n",
      "Iteration 19500 : Loss 3008.9254\n",
      "Iteration 19600 : Loss 3008.8672\n",
      "Iteration 19700 : Loss 3008.8095\n",
      "Iteration 19800 : Loss 3008.7522\n",
      "Iteration 19900 : Loss 3008.6953\n",
      "Iteration 20000 : Loss 3008.6387\n",
      "Iteration 20100 : Loss 3008.5825\n",
      "Iteration 20200 : Loss 3008.5267\n",
      "Iteration 20300 : Loss 3008.4713\n",
      "Iteration 20400 : Loss 3008.4162\n",
      "Iteration 20500 : Loss 3008.3616\n",
      "Iteration 20600 : Loss 3008.3072\n",
      "Iteration 20700 : Loss 3008.2533\n",
      "Iteration 20800 : Loss 3008.1997\n",
      "Iteration 20900 : Loss 3008.1464\n",
      "Iteration 21000 : Loss 3008.0935\n",
      "Iteration 21100 : Loss 3008.0410\n",
      "Iteration 21200 : Loss 3007.9888\n",
      "Iteration 21300 : Loss 3007.9370\n",
      "Iteration 21400 : Loss 3007.8854\n",
      "Iteration 21500 : Loss 3007.8343\n",
      "Iteration 21600 : Loss 3007.7835\n",
      "Iteration 21700 : Loss 3007.7330\n",
      "Iteration 21800 : Loss 3007.6828\n",
      "Iteration 21900 : Loss 3007.6330\n",
      "Iteration 22000 : Loss 3007.5835\n",
      "Iteration 22100 : Loss 3007.5344\n",
      "Iteration 22200 : Loss 3007.4855\n",
      "Iteration 22300 : Loss 3007.4370\n",
      "Iteration 22400 : Loss 3007.3888\n",
      "Iteration 22500 : Loss 3007.3409\n",
      "Iteration 22600 : Loss 3007.2933\n",
      "Iteration 22700 : Loss 3007.2461\n",
      "Iteration 22800 : Loss 3007.1991\n",
      "Iteration 22900 : Loss 3007.1525\n",
      "Iteration 23000 : Loss 3007.1062\n",
      "Iteration 23100 : Loss 3007.0601\n",
      "Iteration 23200 : Loss 3007.0144\n",
      "Iteration 23300 : Loss 3006.9690\n",
      "Iteration 23400 : Loss 3006.9238\n",
      "Iteration 23500 : Loss 3006.8790\n",
      "Iteration 23600 : Loss 3006.8344\n",
      "Iteration 23700 : Loss 3006.7902\n",
      "Iteration 23800 : Loss 3006.7462\n",
      "Iteration 23900 : Loss 3006.7025\n",
      "Iteration 24000 : Loss 3006.6591\n",
      "Iteration 24100 : Loss 3006.6160\n",
      "Iteration 24200 : Loss 3006.5732\n",
      "Iteration 24300 : Loss 3006.5306\n",
      "Iteration 24400 : Loss 3006.4883\n",
      "Iteration 24500 : Loss 3006.4463\n",
      "Iteration 24600 : Loss 3006.4045\n",
      "Iteration 24700 : Loss 3006.3631\n",
      "Iteration 24800 : Loss 3006.3219\n",
      "Iteration 24900 : Loss 3006.2809\n",
      "Iteration 25000 : Loss 3006.2402\n",
      "Iteration 25100 : Loss 3006.1998\n",
      "Iteration 25200 : Loss 3006.1597\n",
      "Iteration 25300 : Loss 3006.1198\n",
      "Iteration 25400 : Loss 3006.0801\n",
      "Iteration 25500 : Loss 3006.0407\n",
      "Iteration 25600 : Loss 3006.0016\n",
      "Iteration 25700 : Loss 3005.9627\n",
      "Iteration 25800 : Loss 3005.9241\n",
      "Iteration 25900 : Loss 3005.8857\n",
      "Iteration 26000 : Loss 3005.8475\n",
      "Iteration 26100 : Loss 3005.8096\n",
      "Iteration 26200 : Loss 3005.7720\n",
      "Iteration 26300 : Loss 3005.7345\n",
      "Iteration 26400 : Loss 3005.6973\n",
      "Iteration 26500 : Loss 3005.6604\n",
      "Iteration 26600 : Loss 3005.6237\n",
      "Iteration 26700 : Loss 3005.5872\n",
      "Iteration 26800 : Loss 3005.5510\n",
      "Iteration 26900 : Loss 3005.5149\n",
      "Iteration 27000 : Loss 3005.4791\n",
      "Iteration 27100 : Loss 3005.4436\n",
      "Iteration 27200 : Loss 3005.4082\n",
      "Iteration 27300 : Loss 3005.3731\n",
      "Iteration 27400 : Loss 3005.3382\n",
      "Iteration 27500 : Loss 3005.3035\n",
      "Iteration 27600 : Loss 3005.2691\n",
      "Iteration 27700 : Loss 3005.2348\n",
      "Iteration 27800 : Loss 3005.2008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 27900 : Loss 3005.1670\n",
      "Iteration 28000 : Loss 3005.1334\n",
      "Iteration 28100 : Loss 3005.1000\n",
      "Iteration 28200 : Loss 3005.0668\n",
      "Iteration 28300 : Loss 3005.0339\n",
      "Iteration 28400 : Loss 3005.0011\n",
      "Iteration 28500 : Loss 3004.9685\n",
      "Iteration 28600 : Loss 3004.9362\n",
      "Iteration 28700 : Loss 3004.9040\n",
      "Iteration 28800 : Loss 3004.8721\n",
      "Iteration 28900 : Loss 3004.8403\n",
      "Iteration 29000 : Loss 3004.8087\n",
      "Iteration 29100 : Loss 3004.7774\n",
      "Iteration 29200 : Loss 3004.7462\n",
      "Iteration 29300 : Loss 3004.7152\n",
      "Iteration 29400 : Loss 3004.6845\n",
      "Iteration 29500 : Loss 3004.6539\n",
      "Iteration 29600 : Loss 3004.6235\n",
      "Iteration 29700 : Loss 3004.5932\n",
      "Iteration 29800 : Loss 3004.5632\n",
      "Iteration 29900 : Loss 3004.5334\n",
      "Iteration 30000 : Loss 3004.5037\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD4CAYAAADsKpHdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAWBElEQVR4nO3df6zddX3H8ee7vS2iqC3SVdI2K2qzpZpZsUEWyeJ0gdItKUuMgT+kYcSaWTJdXCK6ZDh/JLpFt5A5FoyNZXMi80foXB12hM0Zw4+LVigw7BUhtKu0UgQ3nVB474/zufC9937uvae39/Te28/zkZyc7/l8f5zP535v+7qf7+fzPScyE0mSFs11BSRJ84OBIEkCDARJUmEgSJIAA0GSVAzNdQVm6qyzzsq1a9fOdTUkaUG5++67f5KZK2rrFmwgrF27luHh4bmuhiQtKBHxyGTrvGQkSQIMBElSYSBIkgADQZJUGAiSJMBAkCQV0wZCRKyJiNsi4v6IuC8i3lvKPxwRByNib3ls7uzzwYgYiYgHI+KiTvmmUjYSEVd3ys+JiDtK+ZciYulsN1SSNLV+egjHgPdn5nrgfGB7RKwv6/4qMzeUx26Asu5S4LXAJuBvI2JxRCwGPgNcDKwHLusc55PlWK8BngCunKX2TbDzOw/zz9//70EdXpIWrGkDITMPZeZ3y/LPgAeAVVPssgW4MTN/mZk/AkaA88pjJDMfysyngRuBLRERwFuBL5f9dwKXzLA90/qH2x/hG/sODerwkrRgHdcYQkSsBd4A3FGKroqIeyJiR0QsL2WrgEc7ux0oZZOVvwL4aWYeG1dee/9tETEcEcNHjhw5nqqP4XcCSdJEfQdCRJwBfAV4X2Y+BVwHvBrYABwCPjWICnZl5vWZuTEzN65YUf0ojmlFzHKlJOkU0ddnGUXEEnph8IXM/CpAZj7WWf9Z4Ovl5UFgTWf31aWMScofB5ZFxFDpJXS3Hwh7CJI0UT+zjAL4HPBAZn66U352Z7PfB/aV5V3ApRFxWkScA6wD7gTuAtaVGUVL6Q0878relzrfBry97L8VuPnEmjVFe7CLIEk1/fQQ3gy8E7g3IvaWsg/RmyW0AUjgYeDdAJl5X0TcBNxPb4bS9sx8FiAirgJuARYDOzLzvnK8DwA3RsTHgO/RC6CBSewiSNJ40wZCZn4bqn9W755in48DH6+U767tl5kP0ZuFNHARXjKSpBrvVJYkAY0Ggh0ESZqouUAI551KUlVzgQCOIUhSTXOBYP9AkuqaC4QeuwiSNF5zgeAQgiTVNRcI4BiCJNU0Fwj2ECSprrlAAEcQJKmmuUAIgvSakSRN0F4geMlIkqqaCwTwkpEk1TQXCHYQJKmuuUAAp51KUk17geAggiRVtRcIOIYgSTXNBYL9A0mqay4QAO9DkKSK5gLBIQRJqmsuECRJdc0FQuC0U0mqaS8QvGYkSVXNBQJAOvFUkiZoLhDsH0hSXXOBAI4hSFJNc4HgEIIk1TUXCGAPQZJqmguEcBRBkqqaCwRwlpEk1bQXCHYQJKmqvUDAMQRJqmkuEOwgSFJdc4EAfkGOJNU0FwgRmAiSVNFeIHjRSJKqpg2EiFgTEbdFxP0RcV9EvLeUnxkReyJif3leXsojIq6NiJGIuCcizu0ca2vZfn9EbO2UvzEi7i37XBsD/khSp51K0kT99BCOAe/PzPXA+cD2iFgPXA3cmpnrgFvLa4CLgXXlsQ24DnoBAlwDvAk4D7hmNETKNu/q7LfpxJtW50dXSFLdtIGQmYcy87tl+WfAA8AqYAuws2y2E7ikLG8Bbsie24FlEXE2cBGwJzOPZuYTwB5gU1n3ssy8PXtfdnxD51gD4bRTSZrouMYQImIt8AbgDmBlZh4qq34MrCzLq4BHO7sdKGVTlR+olNfef1tEDEfE8JEjR46n6p1jzGg3STrl9R0IEXEG8BXgfZn5VHdd+ct+4H93Z+b1mbkxMzeuWLFi5seZxTpJ0qmir0CIiCX0wuALmfnVUvxYudxDeT5cyg8Cazq7ry5lU5WvrpQPhLOMJKmun1lGAXwOeCAzP91ZtQsYnSm0Fbi5U355mW10PvBkubR0C3BhRCwvg8kXAreUdU9FxPnlvS7vHGsg0kEESZpgqI9t3gy8E7g3IvaWsg8BnwBuiogrgUeAd5R1u4HNwAjwc+AKgMw8GhEfBe4q230kM4+W5fcAnwdOB75RHgPhGIIk1U0bCJn5bSb/CKC3VbZPYPskx9oB7KiUDwOvm64us8X+gSRN1NydyuC0U0mqaS4QBnwTtCQtWM0FAnjJSJJqmgsE+weSVNdcIAAOIkhSRXOB4BCCJNU1FwjgGIIk1TQXCHYQJKmuuUAAhxAkqaa5QPA+BEmqay4QwK/QlKSa5gIh8JKRJNW0FwheMZKkquYCAewhSFJNg4FgF0GSahoMBG9Mk6Sa5gLBMQRJqmsuEMDvVJakmuYCwQ6CJNU1FwiSpLrmAsExBEmqay4QwPsQJKmmuUAIRxEkqaq5QAA/3E6SapoLhAgvGUlSTZOBIEmaqLlAAD+6QpJqmgsEB5Ulqa65QAA/ukKSatoLBDsIklTVXiDgGIIk1TQXCHYQJKmuuUAA7CJIUkVzgRDeiCBJVc0FAthBkKSa5gIhcNqpJNVMGwgRsSMiDkfEvk7ZhyPiYETsLY/NnXUfjIiRiHgwIi7qlG8qZSMRcXWn/JyIuKOUfykils5mAye2Z5BHl6SFq58ewueBTZXyv8rMDeWxGyAi1gOXAq8t+/xtRCyOiMXAZ4CLgfXAZWVbgE+WY70GeAK48kQa1A/7B5I00bSBkJnfAo72ebwtwI2Z+cvM/BEwApxXHiOZ+VBmPg3cCGyJ3gjvW4Evl/13ApccXxOOjx0ESao7kTGEqyLinnJJaXkpWwU82tnmQCmbrPwVwE8z89i48qqI2BYRwxExfOTIkRlX3CEESZpopoFwHfBqYANwCPjUbFVoKpl5fWZuzMyNK1asmNExnHYqSXVDM9kpMx8bXY6IzwJfLy8PAms6m64uZUxS/jiwLCKGSi+hu/3A+I1pkjTRjHoIEXF25+XvA6MzkHYBl0bEaRFxDrAOuBO4C1hXZhQtpTfwvCt78z9vA95e9t8K3DyTOvVd90EeXJIWsGl7CBHxReAtwFkRcQC4BnhLRGygN2HnYeDdAJl5X0TcBNwPHAO2Z+az5ThXAbcAi4EdmXlfeYsPADdGxMeA7wGfm63GTcYxBEmaaNpAyMzLKsWT/qedmR8HPl4p3w3srpQ/RG8W0slhF0GSqpq7UxnsIUhSTXOB4FdoSlJde4FgHkhSVXOBAH64nSTVNBcIdhAkqa65QAA/3E6SapoLBMcQJKmuuUAAp51KUk1zgeC0U0mqay4QwA+3k6Sa5gLBMQRJqmsuEMAxBEmqaS4Q7CFIUl1zgQDehyBJNQ0GQnjJSJIqmgsELxlJUl1zgdBjF0GSxmsuEOwgSFJdc4EATjuVpJrmAsExBEmqay4QwBEESappLhD8cDtJqmsuEMCv0JSkmuYCwTEESaprLhDAMQRJqmkuEAKnnUpSTXuB4DUjSapqLhDAQWVJqmkyECRJEzUZCPYPJGmi5gLBIQRJqmsuEAC7CJJU0Vwg+NEVklTXXCCAHQRJqmkuEBxDkKS6aQMhInZExOGI2NcpOzMi9kTE/vK8vJRHRFwbESMRcU9EnNvZZ2vZfn9EbO2UvzEi7i37XBsn4c4x70OQpIn66SF8Htg0ruxq4NbMXAfcWl4DXAysK49twHXQCxDgGuBNwHnANaMhUrZ5V2e/8e81qwIvGUlSzbSBkJnfAo6OK94C7CzLO4FLOuU3ZM/twLKIOBu4CNiTmUcz8wlgD7CprHtZZt6evT/bb+gcayAi/CwjSaqZ6RjCysw8VJZ/DKwsy6uARzvbHShlU5UfqJRXRcS2iBiOiOEjR47MqOKLIkj7CJI0wQkPKpe/7E/K/7CZeX1mbszMjStWrJjRMSKC58wDSZpgpoHwWLncQ3k+XMoPAms6260uZVOVr66UD0zvkpGJIEnjzTQQdgGjM4W2Ajd3yi8vs43OB54sl5ZuAS6MiOVlMPlC4Jay7qmIOL/MLrq8c6yBWOQYgiRVDU23QUR8EXgLcFZEHKA3W+gTwE0RcSXwCPCOsvluYDMwAvwcuAIgM49GxEeBu8p2H8nM0YHq99CbyXQ68I3yGJhFETxnIkjSBNMGQmZeNsmqt1W2TWD7JMfZAeyolA8Dr5uuHrMlwDEESapo8E7l3n1vjiNI0ljNBcKi5wNhjisiSfNMc4Ew+sEYjiNI0ljNBcKiEgjGgSSN1VwgjI4h2EOQpLGaCwTHECSprrlAGB1DMBAkaazmAmGRg8qSVNVgIDiGIEk1zQXCKONAksZqLhCeH1R+bo4rIknzTIOB0Hv2kpEkjdVcIDz/WUZzXA9Jmm+aCwR7CJJU11wgeKeyJNU1GAhlwTyQpDGaC4QX7kOY44pI0jzTYCD0nr1kJEljNRcIzjKSpLr2AqE8P+c1I0kao7lA8OOvJamuvUAoLU4vGknSGM0FQuAsI0mqaS8QnGUkSVXNBYJjCJJU11wgvPAVmiaCJHU1FwjeqSxJdQ0GQu/ZWUaSNFZzgTB6a9pzfmOaJI3RXCD4WUaSVNdgIMT0G0lSg9oLhNJiewiSNFZzgeCdypJU114geB+CJFU1FwjehyBJdc0Fgj0ESao7oUCIiIcj4t6I2BsRw6XszIjYExH7y/PyUh4RcW1EjETEPRFxbuc4W8v2+yNi64k1aWqLSyI8axdBksaYjR7Cb2fmhszcWF5fDdyameuAW8trgIuBdeWxDbgOegECXAO8CTgPuGY0RAZhaHGvyccMBEkaYxCXjLYAO8vyTuCSTvkN2XM7sCwizgYuAvZk5tHMfALYA2waQL0AGFrc6yE886y3KktS14kGQgLfjIi7I2JbKVuZmYfK8o+BlWV5FfBoZ98DpWyy8gkiYltEDEfE8JEjR2ZU4aWlh/DMs/YQJKlr6AT3vyAzD0bErwB7IuK/uiszMyNi1v7nzczrgesBNm7cOKPjjvYQjtlDkKQxTqiHkJkHy/Nh4Gv0xgAeK5eCKM+Hy+YHgTWd3VeXssnKB2Ko3Kr8jGMIkjTGjAMhIl4SES8dXQYuBPYBu4DRmUJbgZvL8i7g8jLb6HzgyXJp6RbgwohYXgaTLyxlA7HEHoIkVZ3IJaOVwNeiN41zCPjHzPzXiLgLuCkirgQeAd5Rtt8NbAZGgJ8DVwBk5tGI+ChwV9nuI5l59ATqNaWh58cQDARJ6ppxIGTmQ8DrK+WPA2+rlCewfZJj7QB2zLQux2PJotFZRl4ykqSu5u5UXjJ6H4I9BEkao7lAeH6WkYPKkjRGc4Ew2kN42h6CJI3RXCAMLRqdZWQPQZK6mguExYucdipJNc0FQkTw4qWL+fnTz851VSRpXmkuEABe9qIlPPV/z8x1NSRpXmkzEE4f4qlfHJvrakjSvNJmILxoCU/+wh6CJHU1GQjLXryUx//3l3NdDUmaV070468XpF975Rnc9uBhRg7/Dy8/fQnwwnctlyfKZzR1Xo+uf2HD0bJR3ZcxbuXYdeP3i0nXjdnuOPabcV2mqoCkU1qTgXDh+ldy3b//kN/59H/MdVUWlG5WjI+NbpBMXNfdb4rkYubB2W8ATv1+4/abhfaOXzv1Mbvrpg/mfrK7n3ifzT8C+qpTX/WeX+3v6yd0EuvzL390AacNLe7jaMenyUB4/Zpl7LrqAvYdfLL3vQjZu0lt9Fa18pKcrLyzrmb8qiSnWDez/ca+X457XT/+xHX979ddOVvHnKq94zce+3Oa/faOlwNv7xT79XHP5Pj3rW7T13H62KbPezj7qdMsbTLlv7/jO04f2/R1nNmpT38b9ReYM9FkIAC8btXLed2ql891NSRp3mhyUFmSNJGBIEkCDARJUmEgSJIAA0GSVBgIkiTAQJAkFQaCJAmA6OcOu/koIo4Aj8xw97OAn8xidebSqdKWU6UdYFvmq1OlLSfajl/NzBW1FQs2EE5ERAxn5sa5rsdsOFXacqq0A2zLfHWqtGWQ7fCSkSQJMBAkSUWrgXD9XFdgFp0qbTlV2gG2Zb46VdoysHY0OYYgSZqo1R6CJGkcA0GSBDQWCBGxKSIejIiRiLh6ruszmYh4OCLujYi9ETFcys6MiD0Rsb88Ly/lERHXljbdExHndo6ztWy/PyK2nqS674iIwxGxr1M2a3WPiDeWn81I2XcgXx01STs+HBEHy3nZGxGbO+s+WOr0YERc1Cmv/s5FxDkRcUcp/1JELB1EO8p7rYmI2yLi/oi4LyLeW8oX4nmZrC0L6txExIsi4s6I+H5px59P9d4RcVp5PVLWr51p+6aUmU08gMXAD4FXAUuB7wPr57pek9T1YeCscWV/AVxdlq8GPlmWNwPfoPd1recDd5TyM4GHyvPysrz8JNT9t4BzgX2DqDtwZ9k2yr4Xn8R2fBj4k8q268vv02nAOeX3bPFUv3PATcClZfnvgD8c4Dk5Gzi3LL8U+EGp80I8L5O1ZUGdm/JzOqMsLwHuKD+/6nsD7wH+rixfCnxppu2b6tFSD+E8YCQzH8rMp4EbgS1zXKfjsQXYWZZ3Apd0ym/IntuBZRFxNnARsCczj2bmE8AeYNOgK5mZ3wKODqLuZd3LMvP27P1ruKFzrJPRjslsAW7MzF9m5o+AEXq/b9XfufLX81uBL5f9uz+TWZeZhzLzu2X5Z8ADwCoW5nmZrC2TmZfnpvxs/6e8XFIeOcV7d8/Vl4G3lboeV/umq1dLgbAKeLTz+gBT/yLNpQS+GRF3R8S2UrYyMw+V5R8DK8vyZO2aT+2drbqvKsvjy0+mq8pllB2jl1g4/na8AvhpZh4bVz5w5VLDG+j9Rbqgz8u4tsACOzcRsTgi9gKH6YXrD6d47+frW9Y/Weo6q//+WwqEheSCzDwXuBjYHhG/1V1Z/gpbkPOFF3LdgeuAVwMbgEPAp+a0NscpIs4AvgK8LzOf6q5baOel0pYFd24y89nM3ACspvcX/a/PbY3aCoSDwJrO69WlbN7JzIPl+TDwNXq/LI+Vrjnl+XDZfLJ2zaf2zlbdD5bl8eUnRWY+Vv4RPwd8lt55geNvx+P0LsMMjSsfmIhYQu8/0C9k5ldL8YI8L7W2LORzk5k/BW4DfnOK936+vmX9y0tdZ/ff/2wPlszXBzBEbxDsHF4YZHntXNerUs+XAC/tLH+H3rX/v2TsAOBflOXfZewA4J2l/EzgR/QG/5aX5TNPUhvWMnYwdtbqzsTBy80nsR1nd5b/mN61W4DXMnZg7yF6g3qT/s4B/8TYwcP3DLAdQe+6/l+PK19w52WKtiyocwOsAJaV5dOB/wR+b7L3BrYzdlD5ppm2b8p6DeqXcD4+6M2e+AG9a3V/Otf1maSOryon7/vAfaP1pHe98FZgP/BvnX+IAXymtOleYGPnWH9Ab5BpBLjiJNX/i/S67M/Qu2555WzWHdgI7Cv7/A3lbvuT1I6/L/W8B9g17j+hPy11epDODJvJfufKeb6ztO+fgNMGeE4uoHc56B5gb3lsXqDnZbK2LKhzA/wG8L1S333An0313sCLyuuRsv5VM23fVA8/ukKSBLQ1hiBJmoKBIEkCDARJUmEgSJIAA0GSVBgIkiTAQJAkFf8PTF1zCKocMCUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 최종 모델 학습 \n",
    "# 학습률 0.01, epoch 30001일 때 loss가 가장 작음\n",
    "LEARNING_RATE = 0.01\n",
    "losses = []\n",
    "\n",
    "for i in range(1, 30001):\n",
    "    dW, db = gradient(X_train, W, b, y_train)\n",
    "    W -= LEARNING_RATE * dW\n",
    "    b -= LEARNING_RATE * db\n",
    "    L = loss(X_train, W, b, y_train)\n",
    "    losses.append(L)\n",
    "    if i % 100 == 0:\n",
    "        print('Iteration %d : Loss %0.4f' % (i, L))\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835835a5",
   "metadata": {},
   "source": [
    "### (10) test 데이터에 대한 성능 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a58d169b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2593.530763557358"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model(X_test, W, b)\n",
    "mse = loss(X_test, W, b, y_test)\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b2cd0c",
   "metadata": {},
   "source": [
    "### (11) 정답 데이터와 예측한 데이터 시각화하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8cc3eaad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_test[:, 0], y_test)\n",
    "plt.scatter(X_test[:, 0], prediction)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
